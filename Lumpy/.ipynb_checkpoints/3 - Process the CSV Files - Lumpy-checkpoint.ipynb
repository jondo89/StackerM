{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################## LUMPY STACKER - KOLOMELA ##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INSTRUCTIONS:\n",
    "#1. Ensure the CSV's have been pushed to the Mongocloud. \n",
    "\n",
    "#Google Drive Functional Spec\n",
    "#https://docs.google.com/document/d/1DxqJCOKn40OIKej5CaKQ3CNnHxVCJO3I2LFPDUv4V0I/edit#\n",
    "\n",
    "#User Manual\n",
    "#https://docs.google.com/document/d/1qL2m3qvlNFPelwpigaVLsu8i0OQEmeqqw3ghwsF5W4E/edit?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Troubleshooting notes\n",
    "#1. When loading modules, they remain loaded on the page , and deleteing the import can cause unexpected errors.\n",
    "#2. a .env file with passwords etc is required for mongodb access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Required tests.\n",
    "#1.Review and confirm the Weld class and Guage factor calcuation as well as the values . \n",
    "#2.The newly installed gagues have lower guage factors.\n",
    "#3.Work was done to understand the use of filtering , however this was discarded , and not well documented. Review the forward and F/b pass filterin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "machine = \"lumpy\"\n",
    "path = 'C:/Users/jdavi/Desktop/Lumpy April/c/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is a file stream function , and as such requries a \"promise\" for inline code, wont work in the cell as is below. \n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv # add this line\n",
    "import os\n",
    "env_path = Path('.') / '.env'\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "\n",
    "THIS_MACHINE_ENV = os.getenv('THIS_MACHINE_ENV')\n",
    "\n",
    "if(THIS_MACHINE_ENV != machine):\n",
    "    print(\"TEST FAIL : .ENV is incorrect or out of date.\")\n",
    "\n",
    "# A missing or out-of-date .env file can cause errors in a software project.\n",
    "# The .env file stores important configuration information and environment variables needed for the software to function correctly.\n",
    "# If this information is missing or incorrect, the software may not be able to connect to databases or third-party services, or may not be able to access other critical resources.\n",
    "# For example, in a Node.js application, if a database connection string is stored in the .env file and the file is missing or contains an outdated connection string, the application will not be able to connect to the database, causing errors or failure to function correctly.\n",
    "# Similarly, if an API key or other authentication credentials are stored in the .env file and are missing or out-of-date, the software may not be able to authenticate with third-party services or may be denied access to critical resources.\n",
    "# In summary, the .env file is a critical component of many software projects that store sensitive data and configurations. It should be kept up-to-date and include all necessary environment variables to ensure the software functions correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'File Name': ['11-04-2023_00-59-07-090.csv', '11-04-2023_06-59-07-127.csv', '11-04-2023_12-59-07-149.csv', '11-04-2023_18-59-07-171.csv', '12-04-2023_00-59-07-209.csv', '12-04-2023_06-59-07-249.csv', '12-04-2023_12-59-07-273.csv', '12-04-2023_18-59-07-309.csv', '13-04-2023_00-59-07-331.csv', '13-04-2023_06-59-07-370.csv', '13-04-2023_12-59-07-391.csv', '13-04-2023_18-59-07-412.csv', '14-04-2023_00-59-07-449.csv', '14-04-2023_06-59-07-472.csv', '14-04-2023_12-59-07-509.csv', '14-04-2023_18-59-07-549.csv', '15-04-2023_00-59-07-572.csv', '15-04-2023_06-59-07-608.csv', '15-04-2023_12-59-07-630.csv', '15-04-2023_18-59-07-669.csv'], 'Channel': ['Channel 1.1', 'Channel 1.2', 'Channel 1.3', 'Channel 1.4', 'Channel 2.1', 'Channel 2.2', 'Channel 2.3', 'Channel 2.4']}\n",
      "Receive CSV list from Server - Complete\n"
     ]
    }
   ],
   "source": [
    "#CONNECT TO REMOTE MONGODB  \n",
    "import pymongo #import the pymongo pip\n",
    "import datetime #pip module for handling date and time\n",
    "from pymongo import MongoClient #mongodb management\n",
    "import os\n",
    "import certifi #pip module for SSL certificates\n",
    "user = os.getenv('MYSQL_USER')\n",
    "userPC = os.environ['COMPUTERNAME'] # WORK ONLY ON WINDOWS\n",
    "password = os.getenv('MYSQL_PASSWORD')\n",
    "host = os.getenv('MYSQL_HOST')\n",
    "COLLECTIONCSVFILES = os.getenv('COLLECTIONCSVFILES')\n",
    "conn_text = 'mongodb+srv://{}:{}@{}/?retryWrites=true&w=majority'.format(user,password,host)\n",
    "client = MongoClient(conn_text, tlsCAFile=certifi.where())\n",
    "db = client.entangelment\n",
    "collection = db[COLLECTIONCSVFILES]\n",
    "\n",
    "# Adjust the MongoDB query to filter for documents where the \"comment\" field matches the \"path\" variable\n",
    "query = {\"comment\": path}\n",
    "\n",
    "# Use the find() method instead of find_one() to get all matching documents\n",
    "dataset = list(collection.find(query).sort('_id', pymongo.DESCENDING))\n",
    "\n",
    "# Check if the query returned more than one document\n",
    "if len(dataset) > 1:\n",
    "    raise ValueError(\"The query returned more than one document. Please adjust the query to return a single document.\")\n",
    "\n",
    "# If the query returned exactly one document, proceed with the rest of the code\n",
    "dataset = dataset[0]\n",
    "\n",
    "mainarray = {} #initialize the main array\n",
    "\n",
    "#  PRINT THE LIST BOX FOR PROCESSING \n",
    "import ipywidgets as widgets\n",
    "\n",
    "filenamelist = [] #initialize the csv array of filenames\n",
    "channels = ['Channel 1.1', 'Channel 1.2', 'Channel 1.3', 'Channel 1.4', 'Channel 2.1', 'Channel 2.2', 'Channel 2.3', 'Channel 2.4']\n",
    "for i in dataset['date']: #create a list of filenames.\n",
    "    dataloop = i['filename']\n",
    "    filenamelist.append(dataloop)\n",
    "mainarray = {\n",
    "  \"File Name\": filenamelist,\n",
    "  \"Channel\": channels\n",
    "}\n",
    "print(mainarray)\n",
    "print(\"Receive CSV list from Server - Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/jdavi/Desktop/Lumpy April/c/11-04-2023_00-59-07-090.csv 0/20\n",
      "C:/Users/jdavi/Desktop/Lumpy April/c/11-04-2023_06-59-07-127.csv 1/20\n"
     ]
    }
   ],
   "source": [
    "# Importing Modules\n",
    "import numpy as np\n",
    "import pandas\n",
    "import glob # module for reading files from a directory\n",
    "import os # module for getting only the filename not the whole path\n",
    "from datetime import datetime\n",
    "from datetime import datetime, timedelta \n",
    "import csv\n",
    "import json\n",
    "    \n",
    "########### GLOBAL VARIABLES \n",
    "Files =[]\n",
    "JSONStructure = []\n",
    "Channels = [] \n",
    "tempVar= []\n",
    "tempVar2= []\n",
    " \n",
    "########  CONSTRUCT JSON OBJECT FOR DATA PROCESSING \n",
    "filecount = len([name for name in os.listdir(path) if os.path.isfile(os.path.join(path, name))])\n",
    "for i in range(len(mainarray['File Name'])):\n",
    "    Files.append(path+mainarray['File Name'][i])\n",
    "    with open(path+mainarray['File Name'][i]) as file:\n",
    "        print(path+mainarray['File Name'][i] +\" \"+ str(i) + \"/\" + str(filecount))\n",
    "        x = pandas.read_csv(file, delimiter=';',low_memory=False)#read in each CSV file\n",
    "        #Extract the start and end times on each CSV file\n",
    "        startTime= str(x.iloc[[0]]['Time'].values[0]) # string extract of start and end times for CSV\n",
    "        EndTime= str(x.iloc[[-5]]['Time'].values[0]) # string extract of start and end times for CSV\n",
    "        if startTime[11:13] =='24':\n",
    "            #print(int(startTime[11:13]))\n",
    "            startTime= str(0)+str(int(startTime[:1])+1)+startTime[2:10]+\"_\"+str(startTime[11:13].replace(\"24\",\"00\"))+\"-\"+startTime[14:]\n",
    "        if EndTime[11:13] =='24':\n",
    "            #print(int(EndTime[11:13])+1)\n",
    "            EndTime= str(0)+ str(int(EndTime[:1])+1)+EndTime[2:10]+\"_\"+str(EndTime[11:13].replace(\"24\",\"00\"))+\"-\"+EndTime[14:]\n",
    "        dateObject = str(startTime[0:10]).replace(\"-\", \" \")\n",
    "        timeObject = str(startTime[11:19]).replace(\"-\", \" \")\n",
    "        total = dateObject+' '+timeObject\n",
    "        dateObject = datetime.strptime(total, '%d %m %Y %H %M %S')\n",
    "        startTimeStr = str(dateObject)\n",
    "        startTime = dateObject\n",
    "        dateObject = str(EndTime[0:10]).replace(\"-\", \" \")\n",
    "        timeObject = str(EndTime[11:19]).replace(\"-\", \" \")\n",
    "        total = dateObject+' '+timeObject\n",
    "        dateObject = datetime.strptime(total, '%d %m %Y %H %M %S')\n",
    "        EndTimeStr = str(dateObject)\n",
    "        EndTime = dateObject\n",
    "        duration = EndTime - startTime\n",
    "        if duration.days == 1:\n",
    "            duration=duration-timedelta(days=1)\n",
    "        if duration.days == -1:\n",
    "            duration=duration+timedelta(days=1) \n",
    "        if duration.days == 2:\n",
    "            duration=duration-timedelta(days=2)\n",
    "        tempVarArray= []\n",
    "        for Channel in mainarray['Channel']:\n",
    "            Channel = Channel.strip('\\\"')\n",
    "            Channels.append(Channel)        \n",
    "            tempVar =   { 'Channel Name':Channel,\n",
    "                        }\n",
    "            tempVarArray.append(tempVar)\n",
    "        tempVar2 = {\n",
    "            \"File\" : {\n",
    "                    'file name':mainarray['File Name'][i],\n",
    "                    \"Duration\": str(duration),\n",
    "                    \"Channel Data\" : tempVarArray,\n",
    "                    }\n",
    "        }               \n",
    "        JSONStructure.append(tempVar2)\n",
    "        #file.close()\n",
    "#NFI\n",
    "Channels = list(dict.fromkeys(Channels)) # remove duplicates inside the channel list\n",
    "print(\"complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Modules \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from math import pi\n",
    "import scipy.fftpack as sf\n",
    "import scipy.signal as sig\n",
    "import rainflow\n",
    "import pandas\n",
    "import glob # module for reading files from a directory\n",
    "import os # module for getting only the filename not the whole path\n",
    "from pylab import*\n",
    "import json\n",
    "import csv\n",
    "\n",
    "#DEFINE GLOBAL VARIABLES\n",
    "Slot1=['Channel 1.1', 'Channel 1.2', 'Channel 1.3', 'Channel 1.4'] \n",
    "Files =[] # empty array to store Files\n",
    "ChannelData = []\n",
    "\n",
    "Corr1 = 0.1840265*0.207 # Channel 1.1 to 1.4 weld class W\n",
    "Corr2 = 0.4784689*0.207 # Channel 2.1 to 2.4 weld class F2    \n",
    "\n",
    "#Create a Low Path Filter\n",
    "# filter response\n",
    "#[W,h] = sig.freqz(b,a, worN=1024)\n",
    "#W = Fs* W/(2*pi)\n",
    "Fs = 50;\n",
    "o = 8;\n",
    "fc = np.array([14]) #Cut Off Frequncy\n",
    "wc = 2*fc/Fs;\n",
    "[b,a] = sig.butter(o, wc, btype = 'lowpass')\n",
    "\n",
    "#BUILD FILE LIST FOR PROCESSING\n",
    "Files=[]\n",
    "for i in range(len(mainarray['File Name'])):\n",
    "    Files.append(mainarray['File Name'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   MODIFY CHANNEL RAW DATA BY WELD CLASS  \n",
    "#Take actual data in CSV , and multiply line by line with the Correction factors.\n",
    "\n",
    "def modifyChannelDataintoStress(Channel):\n",
    "    ChannelData ={\n",
    "                            'actualData' :Channel,\n",
    "                            'stressData' :[]\n",
    "                        }   \n",
    "    if Channel.name in Slot1:\n",
    "        for actualData in Channel :\n",
    "            if str(actualData) !='nan':\n",
    "                modifiedData = float(actualData)*Corr1 \n",
    "                ChannelData['stressData'].append(modifiedData)\n",
    "            else:\n",
    "                print('nan erros here , too many should be investigated.')\n",
    "    else:\n",
    "        for actualData in Channel:\n",
    "            if str(actualData) !='nan':\n",
    "                modifiedData = float(actualData)*Corr2\n",
    "                ChannelData['stressData'].append(modifiedData)\n",
    "            else:\n",
    "                print('nan erros here , too many should be investigated.')\n",
    "    return ChannelData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Package The Filtered Signal\n",
    "def FilteredSignal(Channel):\n",
    "    #Work was done here to understand the use of filtering , however this was discarded , and not well documented.\n",
    "    #At this point , there is no signal filtering , only date as modified in the getchannel data.\n",
    "    #x_filt = sig.filtfilt(b,a, GetChannelData(Channel)) # forward and backward\n",
    "    #x_filt = sig.lfilter(b,a, GetChannelData(Channel)) # Forward only\n",
    "    x_filt =  modifyChannelDataintoStress(Channel) #turn data into stress with correction factors\n",
    "    TimeLength=len(x_filt['stressData'])#get list count\n",
    "    seq=[round(x,1) for x in range(1, TimeLength+1)] #create a running count from 0\n",
    "    hz = 0.02 #1/0.02 = 50Hz capture rate of PMX\n",
    "    x2= [round(x*hz,3) for x in range(0, TimeLength)]#this is create frequency time steps. it is a division of time\n",
    "    dataFrameOfStressData={'count':seq,'timestep':x2,'stressData':x_filt['stressData'],'actualData':x_filt['actualData']}\n",
    "    dataFrameOfStressData= pandas.DataFrame(dataFrameOfStressData, columns=['count','timestep', 'stressData','actualData'])\n",
    "   \n",
    "    #plot stress data\n",
    "    t=dataFrameOfStressData['timestep']\n",
    "    s2=dataFrameOfStressData['stressData']\n",
    "    s3=dataFrameOfStressData['actualData']\n",
    " \n",
    "    #plt.plot(t, s2,t, s3) # plot the filtered results.  \n",
    "    #plt.xlabel(\"Time (s)\")\n",
    "    #plt.ylabel(\"Stress (MPa)\")\n",
    "    return dataFrameOfStressData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RainFlow Counting\n",
    "def RainFlow(Filtered, Channel):\n",
    "    RainFlow_Results =[] #Initialize an empty list to store the results of Rainflow Algorithm\n",
    "    x = Filtered.timestep.tolist()\n",
    "    y = round(Filtered['stressData'],3).tolist()\n",
    "    # Rainflow Count Algorithm\n",
    "    # Function count_cycles returns a sorted list of the load ranges and the corresponding number of cycles\n",
    "    # 3 decimal digits is used for precision\n",
    "    # Mean, Range , Cycle Count list use to store each point in 3-Dimension\n",
    "    damage1 = 0 #50 % damage\n",
    "    damage2 = 0 # 97.7 % damage\n",
    "    # Detailed output, like cycle lows, highs or means, use extract_cycles\n",
    "    for rng, mean, count, i_start, i_end in rainflow.extract_cycles(y): \n",
    "        # compute the 50 % damage \n",
    "        if rng != 0: # if the range is zero no computation\n",
    "            if Channel in Slot1 :\n",
    "                Nfw=10**(11.5662-3*np.log10(rng))\n",
    "                Ni = (count/Nfw)*100\n",
    "                damage1=damage1+Ni\n",
    "            else:\n",
    "                Nfw=10**(12.0900-3*np.log10(rng))\n",
    "                Ni = (count/Nfw)*100\n",
    "                damage1=damage1+Ni\n",
    "            # Compute the 97.7 % damage\n",
    "            if Channel in Slot1 :\n",
    "                Nfw=10**(11.5662-0.1846*2-3*np.log10(rng))\n",
    "                Ni = (count/Nfw)*100\n",
    "                damage2=damage2+Ni\n",
    "            else:\n",
    "                Nfw=10**(12.090-0.2279*2-3*np.log10(rng))\n",
    "                Ni = (count/Nfw)*100\n",
    "                damage2=damage2+Ni\n",
    "    print(Channel , ' dmg @50% prob. survival : ',damage1,' %', ' dmg @97.7% prob. survival : ',damage2,' %') \n",
    "    RainFlow_Results.append(damage1) # 50 % damage\n",
    "    RainFlow_Results.append(damage2) # 97.7 % damage\n",
    "    return RainFlow_Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running the Script Section\n",
    "#Reading CSV Files From Folder\n",
    "#RawData = pandas.DataFrame() # Initialize an empty dataframe \n",
    "count = 0\n",
    "for file_nameShort in Files:\n",
    "    print(file_nameShort +\" \"+ str(count) + \"/\" + str(filecount))\n",
    "    count = count+1\n",
    "    with open(path+file_nameShort) as file:\n",
    "        dataofCSV = pandas.read_csv(file, delimiter=';',low_memory=False, on_bad_lines='skip')\n",
    "        dataofCSV = dataofCSV.drop(dataofCSV.tail(3).index)\n",
    "        # Calling the function on each Filename\n",
    "        for Channel in Channels:\n",
    "            dataframeOfFilteredData = FilteredSignal(dataofCSV[Channel]) # call the filtering the data and output a dataframe\n",
    "            ab2 = RainFlow(dataframeOfFilteredData,Channel) # using the filtered data return a list of damages \n",
    "            #Assign outputs to JSONStructure\n",
    "            #1. Find the File name in the JSONStructure we are looking for.\n",
    "            for i in JSONStructure:\n",
    "                if i['File']['file name'] == file_nameShort:\n",
    "                    #2. Find the Channel Name We are looking for in the JSONStructure.\n",
    "                    for j in i['File']['Channel Data']:\n",
    "                        if j['Channel Name'] == Channel:\n",
    "                            #3. Assign the Data\n",
    "                            j['50 Damage'] = ab2[0]\n",
    "                            j['97 Damage'] = ab2[1]\n",
    "                            j['Minimum Stress']=dataframeOfFilteredData.min()[2]\n",
    "                            j['Maximum Stress']=dataframeOfFilteredData.max()[2] \n",
    "            file.close()\n",
    "print(\"complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#POST PROCESSING OF DATASET\n",
    "dataset = []\n",
    "channelList = ['Channel 1.1', 'Channel 1.2','Channel 1.3','Channel 1.4','Channel 2.1','Channel 2.2','Channel 2.3','Channel 2.4']\n",
    "damage97 =[]\n",
    "damage50 =[]\n",
    "for j in range(len(Channels)):\n",
    "    dataset.append({\n",
    "        'name' : Channels[j],\n",
    "        '50 Damage' : 0 ,\n",
    "        '97 Damage':0,\n",
    "        'Stress Range':[],\n",
    "        'Maximum Stress Range':0,\n",
    "    })  \n",
    "for i in range(len(JSONStructure)):#Loop throguh the JSON object , and view each file name.\n",
    "    for j in range(len(JSONStructure[i]['File']['Channel Data'])): #Loop through the channel data to view each channel dataset.      \n",
    "        ChannelName = JSONStructure[i]['File']['Channel Data'][j]['Channel Name'] # define a variable to channelname.\n",
    "        for j in range(len(Channels)):#Loop through the summary data array and find the place to put the data.\n",
    "            if ChannelName == dataset[j]['name']:#Check that the right channel has been found.\n",
    "                dataset[j]['50 Damage'] = dataset[j]['50 Damage'] + JSONStructure[i]['File']['Channel Data'][j]['50 Damage']#Add the data sets together into the storage array.\n",
    "                dataset[j]['97 Damage'] = dataset[j]['97 Damage'] + JSONStructure[i]['File']['Channel Data'][j]['97 Damage']#Add the data sets together into the storage array.\n",
    "                dataset[j]['Stress Range'].append(JSONStructure[i]['File']['Channel Data'][j]['Maximum Stress'] -JSONStructure[i]['File']['Channel Data'][j]['Minimum Stress']) # Calculate stress range \n",
    "                damage50.append(dataset[j]['50 Damage'])\n",
    "                damage97.append(dataset[j]['97 Damage'])\n",
    "#print(dataset)                \n",
    "for i in range(len(dataset)):\n",
    "    dataset[i]['Maximum Stress Range']  = max(dataset[i]['Stress Range']) \n",
    "    dataset[i].pop('Stress Range')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Plot 50 Damage\n",
    "ax1.set_title('50 Damage')\n",
    "ax1.set_xlabel('Channel')\n",
    "ax1.set_ylabel('Damage')\n",
    "for i, data in enumerate(dataset):\n",
    "    name = data[\"name\"]\n",
    "    damage_50 = data[\"50 Damage\"]\n",
    "    ax1.bar(i, damage_50, label=name)\n",
    "ax1.legend()\n",
    "\n",
    "# Plot 97 Damage\n",
    "ax2.set_title('97 Damage')\n",
    "ax2.set_xlabel('Channel')\n",
    "ax2.set_ylabel('Damage')\n",
    "for i, data in enumerate(dataset):\n",
    "    name = data[\"name\"]\n",
    "    damage_97 = data[\"97 Damage\"]\n",
    "    ax2.bar(i, damage_97, label=name)\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell is used to visually compare the local and cloud servers.\n",
    "import pymongo\n",
    "import datetime\n",
    "from pymongo import MongoClient\n",
    "import os\n",
    "import pprint\n",
    "import pandas as pd\n",
    "import certifi\n",
    "\n",
    "# Environment variable for MongoDB collection\n",
    "COLLECTION_PROCCESSED_DATA = os.getenv('COLLECTION_PROCCESSED_DATA')\n",
    "\n",
    "# Connect to the remote MongoDB database\n",
    "user = os.getenv('MYSQL_USER')\n",
    "password = os.getenv('MYSQL_PASSWORD')\n",
    "remote_conn_text = f'mongodb+srv://{user}:{password}@entangelment.2vytk.mongodb.net/?retryWrites=true&w=majority'\n",
    "remote_client = MongoClient(remote_conn_text, tlsCAFile=certifi.where())\n",
    "remote_db = remote_client.entangelment\n",
    "remote_collection = remote_db[COLLECTION_PROCCESSED_DATA]\n",
    "\n",
    "# Fetch last 10 entries from the remote MongoDB\n",
    "remote_last_entries = remote_collection.find().sort(\"_id\", pymongo.DESCENDING).limit(10)\n",
    "\n",
    "# Connect to the local MongoDB database\n",
    "local_conn_text = 'mongodb://localhost:27017'\n",
    "local_client = MongoClient(local_conn_text)\n",
    "local_db = local_client.entangelment\n",
    "local_collection = local_db[COLLECTION_PROCCESSED_DATA]\n",
    "\n",
    "# Fetch last 10 entries from the local MongoDB\n",
    "local_last_entries = local_collection.find().sort(\"_id\", pymongo.DESCENDING).limit(10)\n",
    "\n",
    "# Print \"description\" data from the remote database\n",
    "print(\"Description from the Remote DB:\")\n",
    "for entry in remote_last_entries:\n",
    "    print(entry.get('Description', \"N/A\"))\n",
    "\n",
    "# Print \"description\" data from the local database\n",
    "print(\"\\nDescription from the Local DB:\")\n",
    "for entry in local_last_entries:\n",
    "    print(entry.get('Description', \"N/A\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo #import the pymongo pip\n",
    "import datetime\n",
    "#mongodb://heroku_6x00zflw:muikokfevp1h13d5pu0ph74p21@ds161109.mlab.com:61109/heroku_6x00zflw\n",
    "from pymongo import MongoClient\n",
    "user = os.getenv('MYSQL_USER')\n",
    "userPC = os.environ['COMPUTERNAME'] # WORK ONLY ON WINDOWS\n",
    "password = os.getenv('MYSQL_PASSWORD')\n",
    "host = os.getenv('MYSQL_HOST')\n",
    "COLLECTION_PROCCESSED_DATA = os.getenv('COLLECTION_PROCCESSED_DATA')\n",
    "conn_text = 'mongodb+srv://{}:{}@{}/?retryWrites=true&w=majority'.format(user,password,host)\n",
    "client = MongoClient(conn_text, tlsCAFile=certifi.where())\n",
    "db = client.entangelment\n",
    "collection = db[COLLECTION_PROCCESSED_DATA]\n",
    "post = {\"author\": userPC,\n",
    "        \"timestamp\": datetime.datetime.utcnow(),\n",
    "        \"Description\":path,\n",
    "        'data':JSONStructure,\n",
    "        'summary':dataset,\n",
    "        \n",
    "       }\n",
    "post_id = collection.insert_one(post)\n",
    "print('complete ' + path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Notes\n",
    "# Should there be issues with the array struture above, here is he structure.:\n",
    "example = {\n",
    "  \"File Name\": [\n",
    "    \"03-05-2020_02-37-05-588.csv\"\n",
    "  ],\n",
    "  \"Channel\": [\n",
    "    \"Channel 1.1\",\n",
    "    \"Channel 1.2\",\n",
    "    \"Channel 1.3\",\n",
    "    \"Channel 1.4\",\n",
    "    \"Channel 2.1\",\n",
    "    \"Channel 2.2\",\n",
    "    \"Channel 2.3\",\n",
    "    \"Channel 2.4\"\n",
    "  ]\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
